{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130b7f2e",
   "metadata": {},
   "source": [
    "# üß† Softmax Activation Function and Classifier\n",
    "This notebook explains the softmax function, its mathematical formulation, and implements a softmax classifier from scratch using NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92109ad2",
   "metadata": {},
   "source": [
    "## üìê Softmax Formula\n",
    "Given a vector $\\mathbf{z} \\in \\mathbb{R}^K$, the softmax function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "- Outputs probabilities that sum to 1.\n",
    "- Used in the output layer of neural networks for multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd2331e",
   "metadata": {},
   "source": [
    "## üí° Intuition\n",
    "- Converts raw logits into a probability distribution.\n",
    "- Emphasizes the largest values, making the output interpretable as probabilities.\n",
    "- Ensures numerical stability by subtracting the max logit before exponentiating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Example\n",
    "z = np.array([[2.0, 1.0, 0.1]])\n",
    "print(\"Softmax Output:\", softmax(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ac27f",
   "metadata": {},
   "source": [
    "## üîÅ Gradient and Training\n",
    "We use **cross-entropy loss** combined with softmax for training.\n",
    "The loss function for true labels $y \\in \\{0,1,\\dots,K-1\\}$ and predicted probs $\\hat{Y}$ is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{m} \\sum_{i=1}^m \\log(\\hat{Y}_{i, y_i})\n",
    "$$\n",
    "\n",
    "### Gradient of the loss w.r.t logits:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial Z} = \\hat{Y} - Y_{\\text{true}}\n",
    "$$\n",
    "where $Y_{\\text{true}}$ is a one-hot representation.\n",
    "\n",
    "We use this in gradient descent to update weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc211b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier:\n",
    "    def __init__(self, lr=0.1, n_iter=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        z -= np.max(z, axis=1, keepdims=True)\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.num_classes = np.max(y) + 1\n",
    "        self.weights = np.zeros((n, self.num_classes))\n",
    "        self.bias = np.zeros((1, self.num_classes))\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            logits = np.dot(X, self.weights) + self.bias\n",
    "            probs = self._softmax(logits)\n",
    "\n",
    "            # Gradient computation\n",
    "            probs[np.arange(m), y] -= 1\n",
    "            probs /= m\n",
    "\n",
    "            dw = np.dot(X.T, probs)\n",
    "            db = np.sum(probs, axis=0, keepdims=True)\n",
    "\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        logits = np.dot(X, self.weights) + self.bias\n",
    "        probs = self._softmax(logits)\n",
    "        return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7540e7f7",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "- Softmax converts logits to probabilities.\n",
    "- Cross-entropy loss provides an effective gradient for multiclass learning.\n",
    "- The classifier updates weights using the softmax derivative without needing one-hot labels."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
