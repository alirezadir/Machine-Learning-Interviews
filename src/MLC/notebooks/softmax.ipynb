{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10956ca",
   "metadata": {},
   "source": [
    "# üß† Softmax Classifier with Full Gradient Explanation\n",
    "This notebook includes formulas, training steps, and a complete classifier implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c60dd7",
   "metadata": {},
   "source": [
    "## üìê Step-by-Step Explanation\n",
    "\n",
    "### 1. Compute Logits (Linear Transformation)\n",
    "$$\n",
    "Z = XW + b\n",
    "$$\n",
    "\n",
    "- \\(X\\): input matrix \\(m \\times n\\)\n",
    "- \\(W\\): weights \\(n \\times K\\)\n",
    "- \\(b\\): bias \\(1 \\times K\\)\n",
    "- \\(Z\\): logits \\(m \\times K\\)\n",
    "\n",
    "```python\n",
    "logits = np.dot(X, self.weights) + self.bias\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Apply Softmax Function\n",
    "Converts logits to probabilities:\n",
    "\n",
    "$$\n",
    "\\hat{Y}_{i,j} = \\frac{e^{Z_{i,j}}}{\\sum_{k=1}^{K} e^{Z_{i,k}}}\n",
    "$$\n",
    "\n",
    "```python\n",
    "probs = self._softmax(logits)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Compute Softmax Derivative (Loss Gradient)\n",
    "Subtract 1 for the correct class:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial Z} = \\hat{Y} - Y_{\\text{true}}\n",
    "$$\n",
    "\n",
    "```python\n",
    "probs[np.arange(m), y] -= 1\n",
    "probs /= m\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Calculate Gradients for Weights and Bias\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = X^\\top (\\hat{Y} - Y_{\\text{true}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = \\sum (\\hat{Y} - Y_{\\text{true}})\n",
    "$$\n",
    "\n",
    "```python\n",
    "dw = np.dot(X.T, probs)\n",
    "db = np.sum(probs, axis=0, keepdims=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Perform Gradient Descent Updates\n",
    "\n",
    "$$\n",
    "W \\leftarrow W - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W},\\quad b \\leftarrow b - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b}\n",
    "$$\n",
    "\n",
    "```python\n",
    "self.weights -= self.lr * dw\n",
    "self.bias -= self.lr * db\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d2e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxClassifier:\n",
    "    def __init__(self, lr=0.1, n_iter=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        z -= np.max(z, axis=1, keepdims=True)\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.num_classes = np.max(y) + 1\n",
    "        self.weights = np.zeros((n, self.num_classes))\n",
    "        self.bias = np.zeros((1, self.num_classes))\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            logits = np.dot(X, self.weights) + self.bias\n",
    "            probs = self._softmax(logits)\n",
    "\n",
    "            # Gradient computation\n",
    "            probs[np.arange(m), y] -= 1\n",
    "            probs /= m\n",
    "\n",
    "            dw = np.dot(X.T, probs)\n",
    "            db = np.sum(probs, axis=0, keepdims=True)\n",
    "\n",
    "            # Gradient Descent update\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        logits = np.dot(X, self.weights) + self.bias\n",
    "        probs = self._softmax(logits)\n",
    "        return np.argmax(probs, axis=1)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
