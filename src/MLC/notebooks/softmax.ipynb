{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b815188e",
   "metadata": {},
   "source": [
    "# üß† Softmax Activation Function\n",
    "This notebook explains the Softmax function with formula, intuition, and code implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb57e59",
   "metadata": {},
   "source": [
    "## üìê Formula\n",
    "For a vector $\\mathbf{z} \\in \\mathbb{R}^k$ (logits), the softmax function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}}\n",
    "$$\n",
    "\n",
    "This turns a vector of raw scores into probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268e21e",
   "metadata": {},
   "source": [
    "## üí° Intuition\n",
    "- Converts raw logits into a probability distribution.\n",
    "- The output values lie in the range (0, 1) and sum to 1.\n",
    "- Emphasizes larger values while still considering all elements.\n",
    "- Commonly used in the output layer for multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc5a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"Basic softmax.\"\"\"\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n",
    "\n",
    "def stable_softmax(z):\n",
    "    \"\"\"Numerically stable softmax.\"\"\"\n",
    "    z_max = np.max(z, axis=-1, keepdims=True)\n",
    "    exp_z = np.exp(z - z_max)\n",
    "    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n",
    "\n",
    "# Example usage:\n",
    "z = np.array([2.0, 1.0, 0.1])\n",
    "print(\"Softmax:\", softmax(z))\n",
    "print(\"Stable Softmax:\", stable_softmax(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b03fad",
   "metadata": {},
   "source": [
    "## üìâ Gradient (Jacobian)\n",
    "The derivative of softmax is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma_i}{\\partial z_j} = \n",
    "\\begin{cases}\n",
    "\\sigma_i (1 - \\sigma_i), & i = j \\\\\n",
    "- \\sigma_i \\sigma_j, & i \\neq j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Or in matrix form:\n",
    "$$\n",
    "J = \\text{diag}(\\sigma) - \\sigma \\sigma^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78add6b5",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "- Softmax maps arbitrary real values to probabilities.\n",
    "- It is commonly used in classification problems.\n",
    "- Always use the numerically stable version in real implementations."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
